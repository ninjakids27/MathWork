# MathWork
my math work in java that I coded throughout my junior in high school.
This contains mostly math im interested in and projects done. This is not really a consisstent github
and I just use it for some slop like physics labs or Stat HW. I've also used this for my ML project from scratch.
Don't use this as a serious library

# ML Project: 
The objective of the project is to make a deep neural network recognize hand written digits. It's mostly a learning experience
and I don't have a lot of patience to optimize the code any further or make it any better.

# TODO LIST:
## November 2025: 
### ML
- [ ] Do Backpropagation (P1)
    - [x] implement box muller transform method for sampling (P1) 
    - [ ] Do He initilization using a normal distribution to initilize the weights (P1)
    - [ ] Write a loss function (P2)
    - [ ] Write a cost function (P2)
    - [ ] Possibly need to review MVC for BP (~P3)
- [ ] Tensor class? do more research in the future and write it up in the notes (~P3)
- [ ] Write the training method and implement parallel processing using CUDA (P2)
- [x] Make more models (P1)

### MatrixOps
- [ ] Single Value decomposition for compressing images (P1)
    - [ ] research Single Value Decomposition (P1)
    - [ ] write notes (P2)

### NEATNESS
- [ ] reorder the structure of a NN to be more readable via a layer class and NN class (P1)
- [ ] Write more docs (P1)
- [ ] Update the notes to what is relevant in the files (P1)

## December 2025: 
### ML
- [x] implement training method to train the NN (P1)
- [x] Do Backpropagation (P1)
    - [x] implement Adam as an optimizer algorithm for the learning rate (P1)
    - [x] implement SGD
    - [x] implement SGD with momentum
    - [x] implement the softmax function (P1)
    - [x] implement box muller transform method for sampling (P1) 
    - [x] Do He initilization using a uniform distribution to initilize the weights (P1)
    - [x] Write a loss function (P2) // post it's the same thing as the cost function so it's just MSE
    - [x] Write a cost function (P2) // lost and cost are used interchangably in the ML community :/
    - [x] Possibly need to review MVC for BP (~P3)
- [x] Debug Backpropagation (P1)
    - [ ] Write a unit test for the gradient (P1) // never doing these ever
    - [ ] Write a unit test for the model (P1) // never doing these ever
- [ ] Tensor class? do more research in the future and write it up in the notes (~P3)
- [ ] Write the training method and implement parallel processing using CUDA (P2)
- [x] Make more models (P1)
- [x] Make a model that can get a 95% accuracy on the MNIST dataset (P2)
### MatrixOps
- [ ] Single Value decomposition for compressing images (P1)
    - [ ] research Single Value Decomposition (P1)
    - [ ] write notes (P2)
- [ ] Fix linear regression (P1)

### UNIT TESTS
- [ ] Write a unit test for the cost function
- [ ] write a unit test for forward propagation


### NEATNESS
- [ ] reorder the structure of a NN to be more readable via a layer class and NN class (P1)
- [ ] Write more docs (P1)
- [ ] Update the notes to what is relevant in the files (P1)
