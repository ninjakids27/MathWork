\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{mathtools}
\usepackage{amsmath}
\title{mathwork}
\author{Adam Ibrahim}
\date{September 2025}

\begin{document}

\maketitle

\section{Introduction}
\par{this is a project that I've started in my Junior year in High school. It's a fun project and would highly reccomend for any one to do. 
The project has pretty notable restrictions I've applied on my self and it's as follows.}
\begin{enumerate}
    \item No imports
    \item Little to no outside help for coding
    \item Make maintainable code
\end{enumerate}
\par{The 2nd and 3rd rule is pretty reasonable for a project. The first rule
some may have some questions and I'll break it down. ``No imports" means no outside library
or code that I haven't written myself. While I've violated this rule I have written atleast 95\% of the code
and know how the 5\% works completely.}\\
\par{While this is a tough project it has led me to understand a whole lot of stuff that
isn't traditionally taught in a book or in a standard library, and has led me to some rabbit holes,
but this should be a successful project? I hope, because I didn't really intend to finish this as I'm just trying to learn.}

\section{MLOPS}
\par{Forward propagation seems to work by just executing the neural network with random weights and biases}

$$ Z=WX+b$$
\par {Where Z is the neuron W is the weight matrix, X is the input vector, and b is the bias, for binary outputs eg (Right or Wrong) you use the sigmoid function to use as the activation function. ReLU is defined as}
$$\text{ReLU}(x) = \begin{dcases}0 & x\leq0 \\ x & x > 0\end{dcases}$$
\par{This is also the sigmoid function which you can use for activation. In the notes I'll be using it
, but in practice I'll be using ReLU. Sigmoid is defined as}
$$\sigma(x) = \frac{1}{1+e^{-x}}$$
\par{There's the tanh function which is short for tan hyperbolic.}
$$\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$$
\par{Now this comes at the cost at computing 4 exponentials. ReLU seems to be the 
cheapest out of them all.}\\

\par{ReLU seems to be the cheapest and seems to get the job done so I'll go with that. }
\par{I wanted to know how to activate a neuron and with this I'll be using the sigmoid for the math but it doesn't matter that much}
$$\mathbf{W} = \begin{pmatrix}
    w_{0,0} & w_{0,1} & \dots & w_{0,n} \\
    w_{1,0} & w_{1,1} & \dots & w_{1,n} \\
    \vdots  & \vdots & \ddots & \vdots \\
    w_{k,0} & w_{k,1} & \dots & w_{k,n} &  
\end{pmatrix}$$
\par{So im going to let W be the weights of our neural network of general size and I'll let I as in input be the input vector which should look like}
$$I = \begin{bmatrix}
    a_{1} \\
    a_{2} \\
    a_{3} \\
    \vdots \\
    a_{n}
\end{bmatrix}$$
\par{with these computing a vector matrix product should be relative ease with the matrix ops file having that method.
Now I'll let another vector with the biases}
$$
B = \begin{bmatrix}
    b_{1} \\
    b_{2} \\
    b_{3} \\
    \vdots \\
    b_{n}
\end{bmatrix}
$$
\par{Now this is where the *magic* comes in the actual ML}

$$a_0^{(1)}=\sigma(\mathbf{W}I+B)$$
$$ a_0^{(1)} = \sigma(w_{0,0}a_{1} + w_{0,1}a_{2} + \dots + w_{0,n}a_{n} +b_{1})$$
\par{this is for activating ONE neuron if we have hundreds of neurons in our hidden layers
then it's going to redo this computation maybe thousands of times. Now we need a cost function or a boolean output}
\\
\par{This is going to be in the form of right and wrong and what is the correct answer is. So let $C(x)$ be our cost function}
$$C(x) = \Sigma(\text{ResultVector}_n - \text{AnswerVector}_n)^2$$
\par{The result vector is the results you get from the output layer and the answervector should be what the output layer should be so it should look like
a bunch of zeros then a 1 and the resultvector should look like a bunch of numbers from 0-1 ergo confidence in the output}
\\
\par{Large values in $C(x)$ is BAD it means that the NN doesn't know anything and it's garbage and we gotta get it low as possible.}
\\
\par{A big thing to know about is the weights initilization,
because how you setup the weights is critical for the execuation of the neural network. We first define the std deviation of the function to be He initilization}
$$\sigma = \sqrt{\frac{2}{n_\text{in}}}$$
\par{And we would do this on a gauss distrubution}
$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}=\frac{1}{\sigma \sqrt{2\pi}e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}}$$
$$\text{Where } \mu = 0$$
\par{Now what we do is that we square the std deviation of He initlization.
We can reduce computation by removing the square root of calculating our std deviation.
Which can help us in the future for floating point precision.}
$$\sigma = \frac{2}{n_\text{in}}$$
\par{Now we can directly substituite it in the normal distrubution}
$$\mathcal{N}(0,\sigma)$$
$$\text{Traditional } \mathcal{N}(0,\sigma^2)$$
\par{There's another way to do this and thats with He Uniform where we draw it from a uniform distrubution.}
$$U(-L,L)$$
$$\text{Where }L = \sqrt{\frac{6}{n_\text{in}}}$$
\par{Now there's questions on what a uniform distrubution is}
\section{MatrixOps}
\par{A matrix is a 2d array in cs or a 2d vector a matrix is denoted by uppercase letters so ``A'' is a matrix but ``a'' is not a matrix. You could also bold it to emphazise it but it's up to you.}
$$\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{pmatrix}$$
\par{This is a 3x3 matrix, you can also have non-square matrices like}
$$\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}$$
\par{this is a 2x3 matrix, you can also have a mxn matrix where m is the number of rows and n is the number of columns. 
You can also have a 1xn matrix which is a row vector or a mx1 matrix which is a column vector.
You can also have a 1x1 matrix which is just a scalar.
With this you can relate this to a system of equations like}
$$f(x) = \begin{dcases}
2x + 3y = 6 \\
4x + 5y = 10
\end{dcases}$$
\par{This can be represented as a matrix equation like}
$$\begin{pmatrix}
2 & 3 \\
4 & 5
\end{pmatrix}$$
\par{This is a coefficent matrix, where it's just the coefficents of the variables in the equations. You can also have an augmented matrix which is the coefficent matrix with the constants on the right side of the equations.
This is called an augmented matrix because it's augmented with the constants.}
$$\begin{pmatrix}
2 & 3 & | & 6 \\
4 & 5 & | & 10
\end{pmatrix}$$
\par{with these augmented matrices we can generalize any system of equations to an augmented or coefficent matrix.
This is useful because we can use row operations to solve the system of equations. Row operations are just operations that we can do to the rows of the matrix to get a solution.
You already used row operations in middle school when you did substitution and gaussian elimination A way to describe a solution for a matrix 
is to put it in row echelon form or reduced row echelon form. Row echelon form is when the leading coefficient of each row is to the right of the leading coefficient of the previous row.
The leading coefficient is the first non-zero number in a row. It looks like}
$$\begin{pmatrix}
1 & 2 & 3 & | & 6 \\
0 & 1 & 4 & | & 5
\end{pmatrix}$$
\par{This is in row echelon form}
$$\begin{pmatrix}
1 & 0 & 0 & | & -14 \\
0 & 1 & 0 & | & 5
\end{pmatrix}$$

\par{This is in reduced row echelon form}\\
\par{To get to row echelon form you can use the following row operations:}
\begin{itemize}
    \item Swap the positions of two rows (interchange)
    \item Multiply a row by a non-zero scalar (scaling)
    \item Add or subtract a multiple of one row to another row (replacement)
\end{itemize}
\par{Matrix multiplication is multiplying two matrices (I know shocking)}
$$
\begin{pmatrix}
    1 & 2 & 3 & 4\\
    2 & 3 & 4 & 5\\
    6 & 7 & 8 & 9
\end{pmatrix}
$$
\section{linear regression}
\par{MLR}
$$(X^{\top}X)^{-1}X^{\top}Y$$
\par{this is also the equation for a linear line}
$$y = mx+b$$
$$m = \frac{n\sum{y}\sum{x^2}-\sum{y}\sum{xy}}{n\sum{x^2}-(\sum{x})^2}$$
$$b=\frac{n\sum{xy}-\sum{x}\sum{y}}{n\sum{x^2}-(\sum{x})^2}$$
\par{The sum for the first dimensional linear equation is actaully the result of plugging $y=mx+b$ 
in the MLR equation. So MLR serves as a one size fits all for any $n$ dimensional }
\section{StatOps}
\par{In StatOps it features functions that I use for my project or for myself in my 
Statistics Class. }\\\\


\par{One notable thing about statistics is that you can find 10 billion different ways
to sample or make a random number. The bedrock of statistics is the normal/gauss distrubution.
Machine learning features weights and biases the problem is that how do you initilize the weights?
Random numbers? If so what range?}\\
\par{This is where the box muller transform makes that descision for us.}
\section{PhysicsOps}
\par{I want the circular orbit of two masses and I want to get the orbit speed of a planet.
In order to do that I'm given star Mass $M$ planet's mass $m$ and radius of the orbit $R$}
$$F_\text{g}=G\frac{m_1m_2}{r^2}$$
$$F_\text{c} = \frac{m_1v^2}{r}$$
$$F_\text{g} = F_\text{c}$$
\par{assuming star planet is stationary and im lazy for the derivation}
$$G\frac{m_1m_2}{r^2} = \frac{m_1v^2}{r}$$
$$\sqrt{G\frac{m_2}{r}}$$

\end{document}
